{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an implementation of a neural network using only raw python and numpy\n",
    "\n",
    "Coding the neural network from scratch is a by no means a good idea for real world applications. However, it is a good way to understand the inner workings of a neural network. This notebook is a good practice for deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f733fe88370>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_data(points, classes):\n",
    "    X = np.zeros((points*classes, 2))\n",
    "    y = np.zeros(points*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(points*class_number, points*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, points) # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2 # theta\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "# Create data with 100 points and 3 classes \n",
    "X, y = create_data(100, 3)\n",
    "# print the different class names\n",
    "print(np.unique(y))\n",
    "# Plot the data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_data` is a function that generates a spiral dataset of 2D points and labels for a specified number of classes and data points per class. It creates two NumPy arrays, X and y, to hold the generated data. For each class, it generates a set of points points with a unique label. The points are generated using polar coordinates with a random offset in theta, and then converted to Cartesian coordinates. Finally, the function returns the X and y arrays as the generated dataset.\n",
    "\n",
    "We notice that the data is not linearly separable. This is a good dataset to test our neural network on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `__init__` method takes in two parameters, `n_inputs` and `n_neurons`. `n_inputs` is the number of input neurons, and `n_neurons` is the number of neurons in this layer. In this method, the weights and biases of the layer are initialized. The weights are initialized randomly using the `np.random.randn` function. The shape of the weights matrix is `(n_inputs, n_neurons)`. The biases are initialized to zeros with a shape of `(1, n_neurons)`. The weights are initialized randomly in order to break symmetry in the network. If the weights are initialized to the same value, then the gradient descent algorithm will not be able to break symmetry and learn the correct weights. \n",
    "\n",
    "* The `forward` method is the implementation of the forward pass for this layer. It takes in inputs, which is a matrix of shape `(batch_size, n_inputs)`. The output of this layer is calculated by taking the dot product of inputs and weights, and then adding the biases. The output of this layer has a shape of `(batch_size, n_neurons)`. The batch size is the number of samples used in each iteration of gradient descent.\n",
    "\n",
    "* The `backward` method in this code is responsible for computing the gradients of the loss with respect to the weights, biases, and inputs of a layer. This is an important step in the process of training a neural network using gradient descent. The `dvalues` parameter represents the derivative of the loss with respect to the output of the layer, which is passed in from the subsequent layer during backpropagation.\n",
    "    1. The method first computes the gradients of the loss with respect to the weights and biases using the chain rule of calculus. The `dweights` are computed as the dot product between the transpose of the input values and the `dvalues`. This results in a matrix of the same shape as the `weights` matrix, representing the gradient of the loss with respect to each weight parameter.\n",
    "\n",
    "    2. Similarly, the `dbiases` are computed by summing the `dvalues` along the rows (axis=0) and keeping the dimensions consistent with the `biases` matrix.\n",
    "    \n",
    "    3. Finally, the gradient of the loss with respect to the inputs is computed as the dot product between the `dvalues` and the transpose of the `weights`. This allows the gradient to be propagated backwards through the network to update the weights of the previous layer.\n",
    "\n",
    "    Overall, the `backward` method is a crucial step in the backpropagation algorithm for training neural networks, and this implementation computes the gradients efficiently using matrix operations.\n",
    "\n",
    "\n",
    "In summary, this class implements a dense layer in a neural network with `n_inputs` input neurons and `n_neurons` output neurons. The weights and biases are randomly initialized, and the forward pass is calculated by taking the dot product of the inputs and weights, and adding biases. The backward pass computes the gradients of the loss with respect to the weights, biases, and inputs of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code defines three activation functions, ReLU, Softmax, and Sigmoid, using NumPy. Each activation function has two main methods: forward and backward.\n",
    "\n",
    "* The `Activation_ReLU` class implements the rectified linear unit (ReLU) activation function. The `forward` method computes the element-wise maximum of the input and zero to generate the output. The `backward` method sets the gradient to zero where the input values were negative.\n",
    "\n",
    "* The `Activation_Softmax` class implements the softmax activation function, which is commonly used for multi-class classification problems. The `forward` method computes the normalized exponential function of the input values to generate the probabilities. The `backward` method computes the Jacobian matrix of the output and multiplies it with the gradient of the loss with respect to the output.\n",
    "\n",
    "* The `Activation_Sigmoid` class implements the sigmoid activation function. The `forward` method computes the sigmoid function of the input to generate the output. The `backward` method computes the derivative of the sigmoid function and multiplies it with the gradient of the loss with respect to the output.\n",
    "\n",
    "In all three classes, the `forward` method saves the input values in the `self.inputs` variable, and the computed output values are saved in the `self.output` variable. In the backward method, the gradient of the loss with respect to the input values is saved in the `self.dinputs` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Stochastic Gradient Descent (SGD) optimizer for a neural network. SGD is a common optimization algorithm used to train neural networks.\n",
    "\n",
    "The Optimizer_SGD class contains the following methods:\n",
    "\n",
    "* `__init__(self, learning_rate=1., decay=0., momentum=0.)`: initializes the optimizer with learning rate, decay, and momentum values. Learning rate is the step size for each update of the model parameters, decay is the rate at which the learning rate decreases over time, and momentum is the parameter that controls the influence of the previous update on the current update. The default values are 1. for learning_rate, 0. for decay, and 0. for momentum.\n",
    "* `pre_update_params(self)`: adjusts the learning rate if decay is specified.\n",
    "* `update_params(self, layer)`: updates the weights and biases of the specified layer using either vanilla SGD updates or momentum updates, depending on whether the momentum parameter is set.\n",
    "* `post_update_params(self)`: increments the iteration count of the optimizer.\n",
    "\n",
    "The update_params method first checks if the layer contains momentum arrays, which are arrays of zeros that are used to store the momentum of the weights and biases. If they don't exist, the arrays are initialized with zeros. The method then calculates the weight and bias updates using either vanilla SGD updates or momentum updates. The weight and bias updates are then used to update the weights and biases of the layer.\n",
    "\n",
    "On a more general note:\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is an optimization algorithm used for minimizing the cost (loss) function in machine learning. It is a stochastic approximation of the gradient descent optimization algorithm, where instead of computing the gradient of the entire training dataset, the algorithm computes the gradient of a random sample (batch) of training data points at each iteration. This is why it is called \"stochastic\", as it introduces randomness into the optimization process.\n",
    "\n",
    "SGD has been found to be very effective in practice because it converges quickly, especially for large datasets. The reason for this is that instead of waiting to compute the gradient of the entire dataset, we can use small batches of data to compute the gradient at each iteration, which leads to faster convergence.\n",
    "\n",
    "In summary, SGD is an optimization algorithm that randomly samples training data at each iteration to compute the gradient of the cost function, and updates the model parameters in the direction of the negative gradient to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Calculates the data loss\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):  \n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_pred = y_pred[range(samples), y_true]\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(y_pred)\n",
    "\n",
    "        # Average loss\n",
    "        data_loss = np.mean(negative_log_likelihoods)\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code firstly defines a base class `Loss` that provides a `calculate` method to calculate the data loss between the predicted output and the ground truth labels.\n",
    "\n",
    "The method `calculate` takes two arguments:\n",
    "\n",
    "* `output`: the output of the model for a given batch of input data. It is a 2D numpy array of shape `(batch_size, num_classes)`.\n",
    "* `y`: the ground truth labels for the input data. It is a 2D numpy array of shape `(batch_size, num_classes)`.\n",
    "\n",
    "The `calculate` method calls the `forward` method, which is not defined in the `Loss` base class. It is implemented on the subclasses, which are the `Loss_CategoricalCrossentropy` and `Loss_BinaryCrossentropy` classes.\n",
    "\n",
    "The `Loss_BinaryCrossentropy` is the implementation of the Binary Cross-Entropy loss function. It inherits from the base `Loss` class and overrides its `forward` and `backward` methods.\n",
    "\n",
    "The `forward` method calculates the loss between the predicted values `y_pred` and the true values `y_true` for a binary classification task. It first clips the predicted values to avoid division by zero errors and then computes the loss as the negative logarithm of the predicted values for the positive class and the negative logarithm of one minus the predicted values for the negative class. It returns the mean loss over all the samples.\n",
    "\n",
    "The `backward` method calculates the gradient of the loss with respect to the inputs `dvalues`. It first clips the inputs to avoid division by zero errors, then it calculates the gradient as the difference between the true values and the predicted values divided by the clipped predicted values times one minus the clipped predicted values, and then it normalizes the gradient by dividing it by the number of outputs per sample and the number of samples. The resulting gradient is stored in the `dinputs` attribute of the class.\n",
    "\n",
    "`Loss_CategoricalCrossentropy` is a class for the categorical cross-entropy loss function, which is commonly used in multiclass classification problems. It is a subclass of the base Loss class.\n",
    "\n",
    "The `forward` method takes in the predicted output values (`y_pred`) and the ground truth label values (`y_true`) and calculates the loss. It first checks if the ground truth labels are in one-hot format (i.e., if the shape of `y_true` is `(batch_size, num_classes)`), and if not, it extracts the probabilities corresponding to the target label for each sample. It then calculates the negative log-likelihoods of these probabilities and returns their average.\n",
    "\n",
    "The `backward` method takes in the derivative values (`dvalues`) and the ground truth label values (`y_true`) and calculates the derivative of the loss with respect to the predicted output values. It first checks if the ground truth labels are in one-hot format and converts them if not. It then calculates the gradient using the derivative values and one-hot ground truth labels, and normalizes the gradient by dividing it by the batch size. The result is stored in the `dinputs` attribute of the class instance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693 (lr: 2\n",
      "epoch: 100, acc: 0.620, loss: 0.658 (lr: 1.8198362147406735\n",
      "epoch: 200, acc: 0.615, loss: 0.658 (lr: 1.6680567139282734\n",
      "epoch: 300, acc: 0.615, loss: 0.658 (lr: 1.539645881447267\n",
      "epoch: 400, acc: 0.615, loss: 0.657 (lr: 1.4295925661186561\n",
      "epoch: 500, acc: 0.615, loss: 0.655 (lr: 1.33422281521014\n",
      "epoch: 600, acc: 0.615, loss: 0.650 (lr: 1.2507817385866167\n",
      "epoch: 700, acc: 0.605, loss: 0.639 (lr: 1.1771630370806356\n",
      "epoch: 800, acc: 0.640, loss: 0.625 (lr: 1.1117287381878822\n",
      "epoch: 900, acc: 0.620, loss: 0.612 (lr: 1.05318588730911\n",
      "epoch: 1000, acc: 0.630, loss: 0.602 (lr: 1.0005002501250624\n",
      "epoch: 1100, acc: 0.650, loss: 0.590 (lr: 0.9528346831824678\n",
      "epoch: 1200, acc: 0.610, loss: 0.575 (lr: 0.9095043201455207\n",
      "epoch: 1300, acc: 0.645, loss: 0.562 (lr: 0.8699434536755111\n",
      "epoch: 1400, acc: 0.655, loss: 0.550 (lr: 0.8336807002917882\n",
      "epoch: 1500, acc: 0.670, loss: 0.537 (lr: 0.8003201280512204\n",
      "epoch: 1600, acc: 0.705, loss: 0.524 (lr: 0.7695267410542516\n",
      "epoch: 1700, acc: 0.730, loss: 0.511 (lr: 0.7410151908114117\n",
      "epoch: 1800, acc: 0.730, loss: 0.499 (lr: 0.7145409074669525\n",
      "epoch: 1900, acc: 0.740, loss: 0.487 (lr: 0.689893066574681\n",
      "epoch: 2000, acc: 0.730, loss: 0.484 (lr: 0.6668889629876625\n",
      "epoch: 2100, acc: 0.725, loss: 0.477 (lr: 0.6453694740238787\n",
      "epoch: 2200, acc: 0.740, loss: 0.466 (lr: 0.6251953735542357\n",
      "epoch: 2300, acc: 0.750, loss: 0.459 (lr: 0.6062443164595333\n",
      "epoch: 2400, acc: 0.765, loss: 0.450 (lr: 0.5884083553986467\n",
      "epoch: 2500, acc: 0.765, loss: 0.443 (lr: 0.5715918833952558\n",
      "epoch: 2600, acc: 0.780, loss: 0.437 (lr: 0.5557099194220616\n",
      "epoch: 2700, acc: 0.790, loss: 0.432 (lr: 0.5406866720735334\n",
      "epoch: 2800, acc: 0.805, loss: 0.425 (lr: 0.5264543300868649\n",
      "epoch: 2900, acc: 0.810, loss: 0.420 (lr: 0.5129520389843549\n",
      "epoch: 3000, acc: 0.815, loss: 0.411 (lr: 0.5001250312578145\n",
      "epoch: 3100, acc: 0.815, loss: 0.407 (lr: 0.4879238838741156\n",
      "epoch: 3200, acc: 0.820, loss: 0.400 (lr: 0.4763038818766373\n",
      "epoch: 3300, acc: 0.825, loss: 0.396 (lr: 0.4652244708071645\n",
      "epoch: 3400, acc: 0.825, loss: 0.390 (lr: 0.4546487838145033\n",
      "epoch: 3500, acc: 0.830, loss: 0.387 (lr: 0.44454323182929534\n",
      "epoch: 3600, acc: 0.835, loss: 0.381 (lr: 0.4348771472059143\n",
      "epoch: 3700, acc: 0.845, loss: 0.374 (lr: 0.42562247286656735\n",
      "epoch: 3800, acc: 0.855, loss: 0.372 (lr: 0.41675349031048137\n",
      "epoch: 3900, acc: 0.855, loss: 0.368 (lr: 0.4082465809348847\n",
      "epoch: 4000, acc: 0.855, loss: 0.365 (lr: 0.4000800160032006\n",
      "epoch: 4100, acc: 0.855, loss: 0.361 (lr: 0.3922337713277113\n",
      "epoch: 4200, acc: 0.855, loss: 0.358 (lr: 0.3846893633391037\n",
      "epoch: 4300, acc: 0.855, loss: 0.353 (lr: 0.37742970371768253\n",
      "epoch: 4400, acc: 0.855, loss: 0.350 (lr: 0.3704389701796629\n",
      "epoch: 4500, acc: 0.850, loss: 0.348 (lr: 0.36370249136206584\n",
      "epoch: 4600, acc: 0.860, loss: 0.343 (lr: 0.3572066440435792\n",
      "epoch: 4700, acc: 0.850, loss: 0.344 (lr: 0.350938761186173\n",
      "epoch: 4800, acc: 0.850, loss: 0.344 (lr: 0.34488704949129156\n",
      "epoch: 4900, acc: 0.850, loss: 0.341 (lr: 0.3390405153415833\n",
      "epoch: 5000, acc: 0.850, loss: 0.339 (lr: 0.33338889814969164\n",
      "epoch: 5100, acc: 0.855, loss: 0.335 (lr: 0.3279226102639777\n",
      "epoch: 5200, acc: 0.860, loss: 0.327 (lr: 0.32263268269075657\n",
      "epoch: 5300, acc: 0.860, loss: 0.326 (lr: 0.3175107159866645\n",
      "epoch: 5400, acc: 0.870, loss: 0.322 (lr: 0.3125488357555868\n",
      "epoch: 5500, acc: 0.855, loss: 0.326 (lr: 0.3077396522541929\n",
      "epoch: 5600, acc: 0.855, loss: 0.324 (lr: 0.30307622367025305\n",
      "epoch: 5700, acc: 0.860, loss: 0.316 (lr: 0.29855202268995373\n",
      "epoch: 5800, acc: 0.860, loss: 0.314 (lr: 0.29416090601559053\n",
      "epoch: 5900, acc: 0.855, loss: 0.313 (lr: 0.2898970865342803\n",
      "epoch: 6000, acc: 0.845, loss: 0.314 (lr: 0.2857551078725532\n",
      "epoch: 6100, acc: 0.845, loss: 0.314 (lr: 0.2817298211015636\n",
      "epoch: 6200, acc: 0.845, loss: 0.310 (lr: 0.27781636338380333\n",
      "epoch: 6300, acc: 0.840, loss: 0.307 (lr: 0.27401013837511984\n",
      "epoch: 6400, acc: 0.845, loss: 0.302 (lr: 0.27030679821597514\n",
      "epoch: 6500, acc: 0.850, loss: 0.300 (lr: 0.26670222696359513\n",
      "epoch: 6600, acc: 0.845, loss: 0.298 (lr: 0.26319252533228055\n",
      "epoch: 6700, acc: 0.880, loss: 0.286 (lr: 0.25977399662293804\n",
      "epoch: 6800, acc: 0.870, loss: 0.287 (lr: 0.25644313373509425\n",
      "epoch: 6900, acc: 0.870, loss: 0.286 (lr: 0.253196607165464\n",
      "epoch: 7000, acc: 0.875, loss: 0.279 (lr: 0.2500312539067383\n",
      "epoch: 7100, acc: 0.875, loss: 0.280 (lr: 0.24694406716878625\n",
      "epoch: 7200, acc: 0.880, loss: 0.277 (lr: 0.24393218685205514\n",
      "epoch: 7300, acc: 0.885, loss: 0.276 (lr: 0.24099289070972407\n",
      "epoch: 7400, acc: 0.880, loss: 0.274 (lr: 0.23812358614120727\n",
      "epoch: 7500, acc: 0.885, loss: 0.272 (lr: 0.23532180256500762\n",
      "epoch: 7600, acc: 0.875, loss: 0.271 (lr: 0.23258518432375858\n",
      "epoch: 7700, acc: 0.885, loss: 0.267 (lr: 0.22991148407862974\n",
      "epoch: 7800, acc: 0.880, loss: 0.264 (lr: 0.22729855665416526\n",
      "epoch: 7900, acc: 0.890, loss: 0.251 (lr: 0.22474435329812337\n",
      "epoch: 8000, acc: 0.895, loss: 0.254 (lr: 0.22224691632403598\n",
      "epoch: 8100, acc: 0.895, loss: 0.246 (lr: 0.21980437410704473\n",
      "epoch: 8200, acc: 0.875, loss: 0.243 (lr: 0.2174149364061311\n",
      "epoch: 8300, acc: 0.865, loss: 0.242 (lr: 0.2150768899881708\n",
      "epoch: 8400, acc: 0.860, loss: 0.238 (lr: 0.2127885945313331\n",
      "epoch: 8500, acc: 0.865, loss: 0.234 (lr: 0.21054847878724076\n",
      "epoch: 8600, acc: 0.865, loss: 0.230 (lr: 0.20835503698301905\n",
      "epoch: 8700, acc: 0.870, loss: 0.227 (lr: 0.20620682544592225\n",
      "epoch: 8800, acc: 0.865, loss: 0.225 (lr: 0.2041024594346362\n",
      "epoch: 8900, acc: 0.870, loss: 0.223 (lr: 0.2020406101626427\n",
      "epoch: 9000, acc: 0.870, loss: 0.218 (lr: 0.2000200020002\n",
      "epoch: 9100, acc: 0.880, loss: 0.211 (lr: 0.19803940984255866\n",
      "epoch: 9200, acc: 0.885, loss: 0.208 (lr: 0.19609765663300324\n",
      "epoch: 9300, acc: 0.885, loss: 0.205 (lr: 0.1941936110301971\n",
      "epoch: 9400, acc: 0.900, loss: 0.196 (lr: 0.19232618521011635\n",
      "epoch: 9500, acc: 0.920, loss: 0.188 (lr: 0.19049433279359937\n",
      "epoch: 9600, acc: 0.930, loss: 0.182 (lr: 0.18869704689121614\n",
      "epoch: 9700, acc: 0.935, loss: 0.181 (lr: 0.1869333582577811\n",
      "epoch: 9800, acc: 0.935, loss: 0.179 (lr: 0.18520233354940274\n",
      "epoch: 9900, acc: 0.925, loss: 0.178 (lr: 0.18350307367648405\n",
      "epoch: 10000, acc: 0.935, loss: 0.176 (lr: 0.18183471224656786\n",
      "validation, acc: 0.895, loss: 0.312\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = create_data(100,2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Sigmoid activation:\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=2, decay=1e-3)\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values, multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = create_data(100, 2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the data loss\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Part in the brackets returns a binary mask - array consisting of\n",
    "# True/False values, multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is implementing a neural network with two dense layers and ReLU and sigmoid activations, using binary cross-entropy loss and stochastic gradient descent optimizer.\n",
    "\n",
    "It first creates a training dataset with 100 samples and 2 features, and reshapes the labels to be a list of lists with one output per output neuron. It then creates the neural network layers and initializes the loss function and optimizer.\n",
    "\n",
    "It trains the model for 10001 epochs, in each epoch performing a forward pass through the layers, calculating the loss and accuracy, and performing a backward pass to update the weights and biases using the optimizer.\n",
    "\n",
    "Finally, it creates a test dataset, performs a forward pass on it, and calculates the accuracy and loss of the model.\n",
    "\n",
    "The model is being trained with stochastic gradient descent and the learning rate is being reduced over time. The training is being performed for a total of 10,000 epochs.\n",
    "\n",
    "The initial accuracy is `0.5` and the initial loss is `0.693`. The accuracy gradually increases and the loss gradually decreases over time. The learning rate is also gradually reduced over time, which is a common technique used in deep learning to ensure that the model converges to the optimal solution.\n",
    "\n",
    "After 10,000 epochs, the validation accuracy is `0.895` and the validation loss is `0.312`. This suggests that the model has learned to generalize well to new data, since the validation accuracy is high and the validation loss is low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# now let's train and validate the neural network with the mnist dataset\n",
    "# we will use the same model as before, but we will use the mnist dataset\n",
    "# instead of the spiral dataset\n",
    "\n",
    "# load the mnist dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the data\n",
    "X_train = X_train.reshape(-1, 28*28)\n",
    "X_test = X_test.reshape(-1, 28*28)\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded the MNIST dataset, reshaped the images into 1D arrays, and normalized the pixel values to be between 0 and 1. The print statements at the end show the shape of the training images and labels.\n",
    "\n",
    "The training images are of shape (60000, 784), which means there are 60,000 images in the training set, each with 784 (28x28) pixels. The training labels are of shape (60000,), which means there are 60,000 corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, acc: 0.086, loss: 2.302 (lr: 1.0)\n",
      "epoch: 0, step: 100, acc: 0.844, loss: 0.453 (lr: 1.0)\n",
      "epoch: 0, step: 200, acc: 0.906, loss: 0.250 (lr: 1.0)\n",
      "epoch: 0, step: 300, acc: 0.930, loss: 0.269 (lr: 1.0)\n",
      "epoch: 0, step: 400, acc: 0.906, loss: 0.235 (lr: 1.0)\n",
      "epoch: 1, step: 0, acc: 0.930, loss: 0.225 (lr: 1.0)\n",
      "epoch: 1, step: 100, acc: 0.938, loss: 0.157 (lr: 1.0)\n",
      "epoch: 1, step: 200, acc: 0.961, loss: 0.144 (lr: 1.0)\n",
      "epoch: 1, step: 300, acc: 0.961, loss: 0.167 (lr: 1.0)\n",
      "epoch: 1, step: 400, acc: 0.922, loss: 0.231 (lr: 1.0)\n",
      "epoch: 2, step: 0, acc: 0.969, loss: 0.110 (lr: 1.0)\n",
      "epoch: 2, step: 100, acc: 0.977, loss: 0.098 (lr: 1.0)\n",
      "epoch: 2, step: 200, acc: 0.945, loss: 0.124 (lr: 1.0)\n",
      "epoch: 2, step: 300, acc: 0.969, loss: 0.134 (lr: 1.0)\n",
      "epoch: 2, step: 400, acc: 0.945, loss: 0.174 (lr: 1.0)\n",
      "epoch: 3, step: 0, acc: 0.977, loss: 0.099 (lr: 1.0)\n",
      "epoch: 3, step: 100, acc: 0.961, loss: 0.090 (lr: 1.0)\n",
      "epoch: 3, step: 200, acc: 0.953, loss: 0.117 (lr: 1.0)\n",
      "epoch: 3, step: 300, acc: 0.977, loss: 0.112 (lr: 1.0)\n",
      "epoch: 3, step: 400, acc: 0.953, loss: 0.150 (lr: 1.0)\n",
      "epoch: 4, step: 0, acc: 0.984, loss: 0.076 (lr: 1.0)\n",
      "epoch: 4, step: 100, acc: 0.969, loss: 0.080 (lr: 1.0)\n",
      "epoch: 4, step: 200, acc: 0.945, loss: 0.117 (lr: 1.0)\n",
      "epoch: 4, step: 300, acc: 0.977, loss: 0.103 (lr: 1.0)\n",
      "epoch: 4, step: 400, acc: 0.953, loss: 0.138 (lr: 1.0)\n",
      "validation, acc: 0.963, loss: 0.131\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create the model\n",
    "dense1 = Layer_Dense(784, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64, 10)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "steps = int(X_train.shape[0] / batch_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "      for step in range(steps):\n",
    "            # calculate the start and end indices\n",
    "            # for the current batch of data\n",
    "            batch_start = step * batch_size\n",
    "            batch_end = (step + 1) * batch_size\n",
    "            \n",
    "            # perform a forward pass of our training data through this layer\n",
    "            dense1.forward(X_train[batch_start:batch_end])\n",
    "            \n",
    "            # perform a forward pass through activation function\n",
    "            # takes the output of first dense layer here\n",
    "            activation1.forward(dense1.output)\n",
    "            \n",
    "            # perform a forward pass through second Dense layer\n",
    "            # takes outputs of activation function of first layer as inputs\n",
    "            dense2.forward(activation1.output)\n",
    "            \n",
    "            # perform a forward pass through activation function\n",
    "            # takes the output of second dense layer here\n",
    "            activation2.forward(dense2.output)\n",
    "            \n",
    "            # calculate the data loss\n",
    "            loss = loss_function.calculate(activation2.output, y_train[batch_start:batch_end])\n",
    "            \n",
    "            # calculate accuracy from output of activation2 and targets\n",
    "            # calculate values along first axis\n",
    "            predictions = np.argmax(activation2.output, axis=1)\n",
    "            accuracy = np.mean(predictions==y_train[batch_start:batch_end])\n",
    "            \n",
    "            if not step % 100:\n",
    "                print(f'epoch: {epoch}, ' +\n",
    "                      f'step: {step}, ' +\n",
    "                      f'acc: {accuracy:.3f}, '+\n",
    "                      f'loss: {loss:.3f} (' +\n",
    "                      f'lr: {optimizer.current_learning_rate})')\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_function.backward(activation2.output, y_train[batch_start:batch_end])\n",
    "            activation2.backward(loss_function.dinputs)\n",
    "            dense2.backward(activation2.dinputs)\n",
    "            activation1.backward(dense2.dinputs)\n",
    "            dense1.backward(activation1.dinputs)\n",
    "\n",
    "            # Update weights and biases\n",
    "            optimizer.pre_update_params()\n",
    "            optimizer.update_params(dense1)\n",
    "            optimizer.update_params(dense2)\n",
    "            optimizer.post_update_params()\n",
    "\n",
    "\n",
    "# validate the model\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains and validates a neural network using the MNIST dataset. The neural network is created with two dense layers, using the ReLU activation function on the first layer and the Softmax activation function on the second layer. The loss function used is categorical cross-entropy, and the optimizer used is stochastic gradient descent (SGD).\n",
    "\n",
    "The training is performed for a specified number of epochs, where each epoch consists of iterating over the entire dataset in batches. For each batch, a forward pass is performed through the neural network, the loss is calculated, and the gradients are propagated backward through the network using backpropagation. The optimizer is then used to update the weights and biases of the network. During training, the accuracy and loss are printed every 100 steps.\n",
    "\n",
    "After training, the model is validated using the test set. The same forward pass procedure is performed, and the accuracy and loss are calculated and printed.\n",
    "\n",
    "In the first epoch, the accuracy increased rapidly from 0.086 to 0.930, and the loss decreased from 2.302 to 0.269. This suggests that the model quickly learned to fit the training data. In the subsequent epochs, the accuracy continued to improve gradually, while the loss decreased more slowly.\n",
    "\n",
    "At the end of the fifth epoch, the model achieved an accuracy of 0.953 on the training data. However, the accuracy on the validation data was higher at 0.963, which suggests that the model did not overfit too much to the training data. The validation loss was also relatively low at 0.131. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 7\n"
     ]
    }
   ],
   "source": [
    "# get the image and its prediction\n",
    "image = X_test[0]\n",
    "prediction = activation2.output[0]\n",
    "\n",
    "# show the image and prediction\n",
    "plt.imshow(image.reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "print(f'prediction: {np.argmax(prediction)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the prediction is correct. The model correctly predicted that the image is a 7."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the process of creating a neural network from scratch can be challenging and time-consuming, but it can also be incredibly rewarding. Through this project, I gained a deeper understanding of how neural networks work and the various components that make them up.\n",
    "\n",
    "Creating a neural network from scratch allowed me to have complete control over every aspect of the model, from the input layer to the output layer. This gave me the freedom to experiment with different architectures and hyperparameters, ultimately leading to a better understanding of how they affect the performance of the model.\n",
    "\n",
    "While it is important to understand the basics of neural networks, creating a fully functioning model from scratch requires a deep understanding of linear algebra, calculus, and programming. It is essential to have a solid foundation in these subjects before attempting to create a neural network from scratch.\n",
    "\n",
    "In conclusion, creating a neural network from scratch is a challenging but rewarding process that can deepen your understanding of neural networks and enhance your skills in linear algebra, calculus, and programming."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
